{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import PIL.Image as pilimg\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch_size , train_data , model , device):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001 , weight_decay=1e-5)\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    PATH = r\"./models/resnet_224.pth\"\n",
    "    pre_loss = 999\n",
    "    for epoch in range(epoch_size):   # 데이터셋을 수차례 반복합니다.\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        for img_i ,label_i in train_data:\n",
    "            img_i ,label_i = img_i.view(-1,3,224,224).to(device), label_i.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img_i)\n",
    "            loss = criterion(outputs , label_i.view(-1).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count +=1\n",
    "            running_loss += loss.item()\n",
    "            if count % 30 == 0:\n",
    "                new_loss = (running_loss / 30)\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, count + 1, new_loss))\n",
    "\n",
    "                if pre_loss > new_loss:\n",
    "                    pre_loss = new_loss\n",
    "                    torch.save(model.state_dict(), PATH)\n",
    "                    print(\"Model svae\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "def test(test_data , model , device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    model.to(device)\n",
    "    for img_i ,label_i in test_data :\n",
    "        img_i ,label_i = img_i.view(-1,3,224,224).to(device), label_i.view(50).to(device)        \n",
    "        outputs = model(img_i)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += label_i.size(0)\n",
    "\n",
    "        label_temp = (label_i.long() == 0)\n",
    "        label_temp2 = (label_i.long() == 1)\n",
    "        pre_temp = (predicted == 0) # positive\n",
    "        for i in range(0,len(pre_temp)):\n",
    "            if (pre_temp[i]==True): # positive\n",
    "                if(label_temp[i]==True):\n",
    "                    true_positive +=1 # \n",
    "                else:\n",
    "                    false_positive +=1\n",
    "            else: # negative\n",
    "                if((label_temp2[i]==False)):\n",
    "                    false_negative +=1\n",
    "                        \n",
    "        correct += (predicted == label_i.long()).sum().item()\n",
    "    print('Accuracy of the network on the 13200 test images: %d %%' % (100 * correct / total))\n",
    "    print('Precision of the network on the 13200 test images: %d %%' % (100 * true_positive / (true_positive+false_positive)))\n",
    "    print('Recall of the network on the 13200 test images: %d %%' % (100 * true_positive / (true_positive+false_negative)))\n",
    "    print(\"True_positive : \" , true_positive)\n",
    "    print(\"False_positive : \" , false_positive)\n",
    "    print(\"False_negative : \" , false_negative)\n",
    "    print(\"Correct : \" , correct)\n",
    "    print(\"Total : \" ,total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyeongy\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dset.ImageFolder(root=r\"./img/train\",\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Scale(224),\n",
    "                               transforms.CenterCrop(224),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                    (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "test_dataset = dset.ImageFolder(root=r\"./img/test\",\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Scale(224),\n",
    "                               transforms.CenterCrop(224),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                    (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset , batch_size=50,\n",
    "                                         shuffle=True,)\n",
    "\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset , batch_size=50,\n",
    "                                         shuffle=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import ToPILImage\n",
    "# print(train_dataloader)\n",
    "# to_img = ToPILImage()\n",
    "\n",
    "# print(len(train_dataloader))\n",
    "# for i, data in enumerate(train_dataloader):\n",
    "#     img = data[0][0,:]\n",
    "#     plt.imshow(to_img(img))\n",
    "#     plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    31] loss: 0.767\n",
      "Model svae\n",
      "[1,    61] loss: 0.478\n",
      "Model svae\n",
      "[1,    91] loss: 0.454\n",
      "Model svae\n",
      "[1,   121] loss: 0.464\n",
      "[1,   151] loss: 0.425\n",
      "Model svae\n",
      "[1,   181] loss: 0.408\n",
      "Model svae\n",
      "[1,   211] loss: 0.381\n",
      "Model svae\n",
      "[1,   241] loss: 0.406\n",
      "[1,   271] loss: 0.356\n",
      "Model svae\n",
      "[1,   301] loss: 0.320\n",
      "Model svae\n",
      "[1,   331] loss: 0.328\n",
      "[1,   361] loss: 0.401\n",
      "[1,   391] loss: 0.318\n",
      "Model svae\n",
      "[1,   421] loss: 0.309\n",
      "Model svae\n",
      "[1,   451] loss: 0.286\n",
      "Model svae\n",
      "[1,   481] loss: 0.262\n",
      "Model svae\n",
      "[1,   511] loss: 0.291\n",
      "[1,   541] loss: 0.247\n",
      "Model svae\n",
      "[1,   571] loss: 0.215\n",
      "Model svae\n",
      "[1,   601] loss: 0.258\n",
      "[1,   631] loss: 0.226\n",
      "[1,   661] loss: 0.219\n",
      "[1,   691] loss: 0.226\n",
      "[1,   721] loss: 0.194\n",
      "Model svae\n",
      "[1,   751] loss: 0.222\n",
      "[1,   781] loss: 0.205\n",
      "[1,   811] loss: 0.162\n",
      "Model svae\n",
      "[1,   841] loss: 0.214\n",
      "[1,   871] loss: 0.206\n",
      "[1,   901] loss: 0.209\n",
      "[1,   931] loss: 0.194\n",
      "[1,   961] loss: 0.167\n",
      "[1,   991] loss: 0.172\n",
      "[1,  1021] loss: 0.173\n",
      "[1,  1051] loss: 0.131\n",
      "Model svae\n",
      "[2,    31] loss: 0.145\n",
      "[2,    61] loss: 0.169\n",
      "[2,    91] loss: 0.156\n",
      "[2,   121] loss: 0.183\n",
      "[2,   151] loss: 0.133\n",
      "[2,   181] loss: 0.164\n",
      "[2,   211] loss: 0.126\n",
      "Model svae\n",
      "[2,   241] loss: 0.195\n",
      "[2,   271] loss: 0.149\n",
      "[2,   301] loss: 0.135\n",
      "[2,   331] loss: 0.141\n",
      "[2,   361] loss: 0.149\n",
      "[2,   391] loss: 0.152\n",
      "[2,   421] loss: 0.189\n",
      "[2,   451] loss: 0.147\n",
      "[2,   481] loss: 0.151\n",
      "[2,   511] loss: 0.116\n",
      "Model svae\n",
      "[2,   541] loss: 0.165\n",
      "[2,   571] loss: 0.139\n",
      "[2,   601] loss: 0.141\n",
      "[2,   631] loss: 0.153\n",
      "[2,   661] loss: 0.123\n",
      "[2,   691] loss: 0.154\n",
      "[2,   721] loss: 0.138\n",
      "[2,   751] loss: 0.143\n",
      "[2,   781] loss: 0.143\n",
      "[2,   811] loss: 0.132\n",
      "[2,   841] loss: 0.138\n",
      "[2,   871] loss: 0.123\n",
      "[2,   901] loss: 0.147\n",
      "[2,   931] loss: 0.134\n",
      "[2,   961] loss: 0.144\n",
      "[2,   991] loss: 0.146\n",
      "[2,  1021] loss: 0.093\n",
      "Model svae\n",
      "[2,  1051] loss: 0.117\n",
      "[3,    31] loss: 0.116\n",
      "[3,    61] loss: 0.113\n",
      "[3,    91] loss: 0.117\n",
      "[3,   121] loss: 0.120\n",
      "[3,   151] loss: 0.147\n",
      "[3,   181] loss: 0.139\n",
      "[3,   211] loss: 0.113\n",
      "[3,   241] loss: 0.132\n",
      "[3,   271] loss: 0.105\n",
      "[3,   301] loss: 0.135\n",
      "[3,   331] loss: 0.129\n",
      "[3,   361] loss: 0.137\n",
      "[3,   391] loss: 0.119\n",
      "[3,   421] loss: 0.116\n",
      "[3,   451] loss: 0.140\n",
      "[3,   481] loss: 0.143\n",
      "[3,   511] loss: 0.143\n",
      "[3,   541] loss: 0.135\n",
      "[3,   571] loss: 0.156\n",
      "[3,   601] loss: 0.113\n",
      "[3,   631] loss: 0.101\n",
      "[3,   661] loss: 0.143\n",
      "[3,   691] loss: 0.123\n",
      "[3,   721] loss: 0.113\n",
      "[3,   751] loss: 0.120\n",
      "[3,   781] loss: 0.088\n",
      "Model svae\n",
      "[3,   811] loss: 0.112\n",
      "[3,   841] loss: 0.097\n",
      "[3,   871] loss: 0.122\n",
      "[3,   901] loss: 0.091\n",
      "[3,   931] loss: 0.113\n",
      "[3,   961] loss: 0.121\n",
      "[3,   991] loss: 0.109\n",
      "[3,  1021] loss: 0.118\n",
      "[3,  1051] loss: 0.112\n",
      "[4,    31] loss: 0.126\n",
      "[4,    61] loss: 0.105\n",
      "[4,    91] loss: 0.111\n",
      "[4,   121] loss: 0.134\n",
      "[4,   151] loss: 0.110\n",
      "[4,   181] loss: 0.098\n",
      "[4,   211] loss: 0.108\n",
      "[4,   241] loss: 0.123\n",
      "[4,   271] loss: 0.112\n",
      "[4,   301] loss: 0.107\n",
      "[4,   331] loss: 0.113\n",
      "[4,   361] loss: 0.093\n",
      "[4,   391] loss: 0.093\n",
      "[4,   421] loss: 0.118\n",
      "[4,   451] loss: 0.137\n",
      "[4,   481] loss: 0.087\n",
      "Model svae\n",
      "[4,   511] loss: 0.096\n",
      "[4,   541] loss: 0.099\n",
      "[4,   571] loss: 0.099\n",
      "[4,   601] loss: 0.115\n",
      "[4,   631] loss: 0.103\n",
      "[4,   661] loss: 0.107\n",
      "[4,   691] loss: 0.087\n",
      "Model svae\n",
      "[4,   721] loss: 0.110\n",
      "[4,   751] loss: 0.103\n",
      "[4,   781] loss: 0.118\n",
      "[4,   811] loss: 0.085\n",
      "Model svae\n",
      "[4,   841] loss: 0.099\n",
      "[4,   871] loss: 0.089\n",
      "[4,   901] loss: 0.086\n",
      "[4,   931] loss: 0.126\n",
      "[4,   961] loss: 0.100\n",
      "[4,   991] loss: 0.093\n",
      "[4,  1021] loss: 0.092\n",
      "[4,  1051] loss: 0.116\n",
      "[5,    31] loss: 0.100\n",
      "[5,    61] loss: 0.088\n",
      "[5,    91] loss: 0.093\n",
      "[5,   121] loss: 0.118\n",
      "[5,   151] loss: 0.083\n",
      "Model svae\n",
      "[5,   181] loss: 0.082\n",
      "Model svae\n",
      "[5,   211] loss: 0.106\n",
      "[5,   241] loss: 0.092\n",
      "[5,   271] loss: 0.084\n",
      "[5,   301] loss: 0.092\n",
      "[5,   331] loss: 0.081\n",
      "Model svae\n",
      "[5,   361] loss: 0.106\n",
      "[5,   391] loss: 0.103\n",
      "[5,   421] loss: 0.101\n",
      "[5,   451] loss: 0.075\n",
      "Model svae\n",
      "[5,   481] loss: 0.103\n",
      "[5,   511] loss: 0.108\n",
      "[5,   541] loss: 0.081\n",
      "[5,   571] loss: 0.132\n",
      "[5,   601] loss: 0.081\n",
      "[5,   631] loss: 0.104\n",
      "[5,   661] loss: 0.081\n",
      "[5,   691] loss: 0.102\n",
      "[5,   721] loss: 0.074\n",
      "Model svae\n",
      "[5,   751] loss: 0.099\n",
      "[5,   781] loss: 0.087\n",
      "[5,   811] loss: 0.108\n",
      "[5,   841] loss: 0.097\n",
      "[5,   871] loss: 0.112\n",
      "[5,   901] loss: 0.083\n",
      "[5,   931] loss: 0.111\n",
      "[5,   961] loss: 0.094\n",
      "[5,   991] loss: 0.078\n",
      "[5,  1021] loss: 0.082\n",
      "[5,  1051] loss: 0.089\n",
      "[6,    31] loss: 0.090\n",
      "[6,    61] loss: 0.082\n",
      "[6,    91] loss: 0.064\n",
      "Model svae\n",
      "[6,   121] loss: 0.080\n",
      "[6,   151] loss: 0.088\n",
      "[6,   181] loss: 0.072\n",
      "[6,   211] loss: 0.087\n",
      "[6,   241] loss: 0.106\n",
      "[6,   271] loss: 0.094\n",
      "[6,   301] loss: 0.081\n",
      "[6,   331] loss: 0.097\n",
      "[6,   361] loss: 0.093\n",
      "[6,   391] loss: 0.095\n",
      "[6,   421] loss: 0.103\n",
      "[6,   451] loss: 0.076\n",
      "[6,   481] loss: 0.073\n",
      "[6,   511] loss: 0.091\n",
      "[6,   541] loss: 0.087\n",
      "[6,   571] loss: 0.073\n",
      "[6,   601] loss: 0.091\n",
      "[6,   631] loss: 0.073\n",
      "[6,   661] loss: 0.080\n",
      "[6,   691] loss: 0.104\n",
      "[6,   721] loss: 0.087\n",
      "[6,   751] loss: 0.062\n",
      "Model svae\n",
      "[6,   781] loss: 0.098\n",
      "[6,   811] loss: 0.089\n",
      "[6,   841] loss: 0.096\n",
      "[6,   871] loss: 0.076\n",
      "[6,   901] loss: 0.081\n",
      "[6,   931] loss: 0.101\n",
      "[6,   961] loss: 0.104\n",
      "[6,   991] loss: 0.089\n",
      "[6,  1021] loss: 0.081\n",
      "[6,  1051] loss: 0.080\n",
      "[7,    31] loss: 0.100\n",
      "[7,    61] loss: 0.066\n",
      "[7,    91] loss: 0.091\n",
      "[7,   121] loss: 0.087\n",
      "[7,   151] loss: 0.094\n",
      "[7,   181] loss: 0.071\n",
      "[7,   211] loss: 0.083\n",
      "[7,   241] loss: 0.084\n",
      "[7,   271] loss: 0.066\n",
      "[7,   301] loss: 0.085\n",
      "[7,   331] loss: 0.071\n",
      "[7,   361] loss: 0.091\n",
      "[7,   391] loss: 0.081\n",
      "[7,   421] loss: 0.079\n",
      "[7,   451] loss: 0.062\n",
      "[7,   481] loss: 0.099\n",
      "[7,   511] loss: 0.066\n",
      "[7,   541] loss: 0.069\n",
      "[7,   571] loss: 0.052\n",
      "Model svae\n",
      "[7,   601] loss: 0.078\n",
      "[7,   631] loss: 0.085\n",
      "[7,   661] loss: 0.108\n",
      "[7,   691] loss: 0.087\n",
      "[7,   721] loss: 0.077\n",
      "[7,   751] loss: 0.059\n",
      "[7,   781] loss: 0.095\n",
      "[7,   811] loss: 0.081\n",
      "[7,   841] loss: 0.075\n",
      "[7,   871] loss: 0.064\n",
      "[7,   901] loss: 0.086\n",
      "[7,   931] loss: 0.091\n",
      "[7,   961] loss: 0.076\n",
      "[7,   991] loss: 0.084\n",
      "[7,  1021] loss: 0.080\n",
      "[7,  1051] loss: 0.089\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50()\n",
    "model.fc = nn.Linear(2048, 2)\n",
    "model.train()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(7, train_dataloader , model , device) # epoch_size , train_data , model , device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PATH = r'./models/googlenet2_224.pth'\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = models.googlenet()\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 13200 test images: 95 %\n",
      "Precision of the network on the 13200 test images: 99 %\n",
      "Recall of the network on the 13200 test images: 92 %\n",
      "True_positive :  24343\n",
      "False_positive :  60\n",
      "False_negative :  2057\n",
      "Correct :  50683\n",
      "Total :  52800\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test(train_dataloader , model , device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# train(5, train_data_loader , model , device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
